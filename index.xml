<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A programmer&#39;s viewpoint</title>
    <link>https://nitinahuja.github.io/</link>
    <description>Recent content on A programmer&#39;s viewpoint</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 17 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://nitinahuja.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Low Earth Orbits - Exploration</title>
      <link>https://nitinahuja.github.io/2023/low-earth-orbits-exploration/</link>
      <pubDate>Sun, 17 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2023/low-earth-orbits-exploration/</guid>
      <description>Introduction In this post, we are going to look at some publicly available data to dig deeper into exploratory data analysis and machine learning techniques. We’ll look at some data from Jonathan McDowell’s Catalog site.
This site has data about man-made objects launched into space and all the debris we have created in near earth orbits.
We’ll try to load the file directly from the site. This allows us to get the most recent data from the internet.</description>
    </item>
    
    <item>
      <title>Generative Art</title>
      <link>https://nitinahuja.github.io/2021/generative-art/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2021/generative-art/</guid>
      <description>library(tidyverse) library(ambient) library(randomcoloR) library(ggforce) disturbance = expand.grid(c = 1:25, r = 1:49) %&amp;gt;% mutate( c = ifelse(r %% 2 == 0, c + 0.5, c), a = 180 * gen_cubic(c, r, frequency = 0.1, seed = 1964) ) %&amp;gt;% filter(c &amp;lt;= 50) ggplot(disturbance) + geom_text(aes(c, r, label = &amp;#34;0&amp;#34;, angle = a, color = a), family = &amp;#34;Times&amp;#34;, size = 16, show.legend = F) + coord_fixed(ratio = 0.5, expand = TRUE) + scale_color_gradient2(low = &amp;#34;orange&amp;#34;, high = &amp;#34;red&amp;#34;, mid = &amp;#34;tomato&amp;#34;) + theme_void() set.</description>
    </item>
    
    <item>
      <title>Vehicle Fuel Economy - exploratory analysis</title>
      <link>https://nitinahuja.github.io/2018/vehicle-fuel-economy-exploratory-analysis/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2018/vehicle-fuel-economy-exploratory-analysis/</guid>
      <description>Introduction In this post, we are going to look at some publicly available data to dig deeper into exploratory data analysis and machine learning techniques. I am going to start by fetching some data from the inter webs, this data is available at the FuelEconomy.gov site. This file has fuel economy data for all cars sold in the United States for several years.
Let&amp;rsquo;s start by loading the libraries we need:</description>
    </item>
    
    <item>
      <title>NPS - Exploratory analysis in R - Text analysis</title>
      <link>https://nitinahuja.github.io/2017/nps-exploratory-text-analysis/</link>
      <pubDate>Sun, 22 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2017/nps-exploratory-text-analysis/</guid>
      <description>NPS analysis NPS - Comment analysis In an [previous post](https://nitinahuja.github.io/2017/nps-exploratory-analysis-in-r/) we performed some EDA on the NPS data we have. Recall that as part of the question about the likelihood of recommending a service or business there is an optional text response about why they picked this score.
Let’s try and see what those responses are all about. We had already performed some sentiment analysis on this text we are now going to attempt to classify this text into topics.</description>
    </item>
    
    <item>
      <title>Heat maps in R</title>
      <link>https://nitinahuja.github.io/2017/heatmaps-in-r/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2017/heatmaps-in-r/</guid>
      <description>Heatmaps Heat maps are invaluable in displaying a large amount of continuous data contained in a 2d matrix. This post is meant to show a way to create a print worthy heat map in R.
Let&amp;rsquo;s start by loading the required packages.
suppressPackageStartupMessages({ library(ggplot2) library(ggthemes) library(viridis) library(scales) library(tidyr) }) Data Our data is from a business that receives sales calls 24x7. Let&amp;rsquo;s read and see what the data looks like. We have observations (count of calls) for each day of the week and each hour of the day.</description>
    </item>
    
    <item>
      <title>Managing managers - some thoughts</title>
      <link>https://nitinahuja.github.io/2017/managing-managers-some-thoughts/</link>
      <pubDate>Wed, 30 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2017/managing-managers-some-thoughts/</guid>
      <description>When you are managing managers, you should have a single focus; they are learning to manage their teams well and as a secondary objective that they are contributing at high levels. There are several experts in this area and I do not claim to be anywhere close; these are just reminders for myself.
Your team learns by modeling you. There is little doubt that what you do, your team will emulate.</description>
    </item>
    
    <item>
      <title>NPS - Exploratory analysis in R</title>
      <link>https://nitinahuja.github.io/2017/nps-exploratory-analysis-in-r/</link>
      <pubDate>Mon, 28 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2017/nps-exploratory-analysis-in-r/</guid>
      <description>NPS analysis What is net promorter score (NPS)? Net Promoter Score or NPS is a customer loyalty metric and was developed by Fred Reichheld and it asks respondents to answer a single question.
How likely are you to recommend this product? The respondents are asked to score between 0 and 10. 10 being &amp;ldquo;most likely&amp;rdquo; to recommend and 0 being &amp;ldquo;least likely&amp;rdquo;.
An additional optional question is asked about why they picked this score and the response to that is usually a text comment.</description>
    </item>
    
    <item>
      <title>Forecasting in R - Philadelphia crime data</title>
      <link>https://nitinahuja.github.io/2016/forecasting-in-r-philadelphia-crime-data/</link>
      <pubDate>Wed, 26 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2016/forecasting-in-r-philadelphia-crime-data/</guid>
      <description>Exploring crime in Philadelphia This is a large and intersting dataset and has data points stretching back over 10 years. Several explorations have pointed out that crime seems to be seasonal and I wanted to explore this with a time series. Assuming that seasonal trends might repeat themselves, I am exploring this using the forecast package and using linear regression to predict trends.
suppressPackageStartupMessages({ library(data.table) library(forecast) library(knitr) }) Data size and structure.</description>
    </item>
    
    <item>
      <title>Access log parsing with Spark and schema extraction from query string</title>
      <link>https://nitinahuja.github.io/2016/access-log-parsing-with-spark-and-schema-extraction-from-query-string/</link>
      <pubDate>Wed, 03 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2016/access-log-parsing-with-spark-and-schema-extraction-from-query-string/</guid>
      <description>I needed to parse server logs and create Spark DataFrames to query information from the query string parameters. My naive version kept throwing errors about mismatched number of fields in schema and those in the row being queried.
It turns out I was dealing with over 350 different query string params across the logs. This could change over time and there was no way I was going to add these programmatically by hand.</description>
    </item>
    
    <item>
      <title>Revenue forecast for seasonal business</title>
      <link>https://nitinahuja.github.io/2016/revenue-forecast-for-seasonal-business/</link>
      <pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2016/revenue-forecast-for-seasonal-business/</guid>
      <description>Revenue forecasting with linear methods. This post highlights linear regression techniques on time series data. We have some weekly sales data from a small business that is expected to have seasonal trends. A bike rental business in a major tourist city. Let&amp;rsquo;s dive into the data. Load up the required packages.
suppressPackageStartupMessages({ library(forecast) }) Load the data, we have two columns with weekly revenue numbers from two different sources. Online and Phone; these correspond to sales online and via phone call to the business.</description>
    </item>
    
    <item>
      <title>On Marketplaces and platforms</title>
      <link>https://nitinahuja.github.io/2014/on-marketplaces-and-platforms/</link>
      <pubDate>Mon, 17 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2014/on-marketplaces-and-platforms/</guid>
      <description>Marketplaces are all the rage these days, in truth, marketplaces have been around since ancient times. A marketplace usually exists to connect sellers of goods or services with buyers. The marketplace itself usually benefits by selling space to sellers or by taking a cut of the transaction.
This post is focused on service marketplaces that are connecting and democratizing human capital in a way that has never been possible. Things like vetting, payments, reviews and support have been researched and A/B tested to an art.</description>
    </item>
    
    <item>
      <title>Finding with find</title>
      <link>https://nitinahuja.github.io/2013/finding-with-find/</link>
      <pubDate>Thu, 11 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://nitinahuja.github.io/2013/finding-with-find/</guid>
      <description>Find is a really versatile utility that can be used to enumerate files of different types, narrow the list by file types, dates, sizes, access times and a whole list of expressions. The output can be formatted with various switches to be csv.
My goal was to list the sizes and access times for all video files in the system. I knew that there was over 3 TB of files but not how recently these were accessed/played.</description>
    </item>
    
  </channel>
</rss>
